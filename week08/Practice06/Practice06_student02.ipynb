{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac7d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "# Matplotlib is a useful plotting library for python \n",
    "import matplotlib.pyplot as plt\n",
    "# This code is to make matplotlib figures appear inline in the\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#Set seed for reproducbility\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d26e2b",
   "metadata": {},
   "source": [
    "### Vision Transformer (ViT)\n",
    "In this excersice, we are going to implement the Visual transformer for the MNIST dataset for image classifcation issue. The Visual Transformer (ViT ) paper : https://arxiv.org/abs/2010.11929\n",
    "\n",
    "\n",
    "The main idea behind ViT is that an image can be seen as a series of patches, which can be treated as tokens (word)in NLP tasks.\n",
    "\n",
    "\n",
    "<img src=\"./ViT.png\" width=\"700\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a4d2d",
   "metadata": {},
   "source": [
    "#### From Images to embedding \n",
    "\n",
    "The differences between image tasks and NLP for transformer is that, the image take each patch as a word in NLP.\n",
    "Particularly, the input image is split into small patches, which are then flattened into sequences of vectors. These vectors are then processed by a transformer encoder, which allows the model to learn interactions between patches through self-attention mechanism. The output of the transformer encoder is then fed into a classification layer that outputs the predicted class of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f04191d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "patch_size = 4\n",
    "num_channels = 1\n",
    "hidden_size = 512\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        #YOUR CODE##\n",
    "        self.num_patches = \n",
    "        #YOUR CODE##\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6588fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbe99589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:15<00:00, 620747.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 20885464.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 646611.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 3092116.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=True)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfa08f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape is: torch.Size([64, 1, 28, 28])\n",
      "After embedding: torch.Size([64, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "#check the output shape after embedding\n",
    "pe = PatchEmbeddings()\n",
    "for i, (images, labels) in enumerate(train_loader): \n",
    "    if i == 0 :\n",
    "        print(\"Images shape is:\", images.shape)\n",
    "        print(\"After embedding:\", pe(images).shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80745ad6",
   "metadata": {},
   "source": [
    "### Add postion embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d55b1c",
   "metadata": {},
   "source": [
    "After the patches are converted to a sequence of embeddings, the [CLS] token is added to the beginning of the sequence, it will be used later in the classification layer to classify the image. The [CLS] token’s embedding is learned during training.\n",
    "\n",
    "As patches from different positions may contribute differently to the final predictions, we also need a way to encode patch positions into the sequence. We’re going to use learnable position embeddings to add positional information to the embeddings. This is similar to how position embeddings are used in Transformer models for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2fbc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings()\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, hidden_size))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1\n",
    "        ##YOUR CODE##\n",
    "        x = \n",
    "        ##YOUR CODE##\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6d3dc",
   "metadata": {},
   "source": [
    "### Build Transformer encoder and classifier\n",
    "\n",
    "Now, try to use the transformer encoder and build a classifier for the MNIST classification, as well as establih the training process as we did in the last excerise class for this task. Compare your results with other classmates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e990b4c6",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "\n",
    "https://tintn.github.io/Implementing-Vision-Transformer-from-Scratch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
