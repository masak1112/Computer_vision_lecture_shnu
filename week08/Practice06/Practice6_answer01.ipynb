{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dac5f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "# Matplotlib is a useful plotting library for python \n",
    "import matplotlib.pyplot as plt\n",
    "# This code is to make matplotlib figures appear inline in the\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#Set seed for reproducbility\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0b073",
   "metadata": {},
   "source": [
    "### Establish Transformer architecture\n",
    "\n",
    "The objectives of this excersie is to:\n",
    "\n",
    "   * Know how to implement the transformer architecture by PyTorch\n",
    "   * Know how to implement the visiaul transformer architecture by PyTorch, and use it for image classifcation task\n",
    "   * Homework (Try to implement Masked Autoencoder by transformer architecture for image classification (MNIST))  <span style=\"color:red\">(Deadline is 28th April, 2024 18:00)</span>  \n",
    "\n",
    "\n",
    "In this course, you will need to implement Transformer architecture based on lecture given last time, as showed below:\n",
    "    \n",
    "<img src=\"./transformer.png\" width=\"400\" height=\"200\">\n",
    "   \n",
    "   \n",
    "And you need to get familar with the following four components of transformer architecture:\n",
    "   * Word Embedding\n",
    "   * Encoder Decoder Models \n",
    "   * Attentions \n",
    "   * Position encoding\n",
    "   \n",
    "   \n",
    "\n",
    "Now, we need to define the basic building blocks: Embedding,Positional Encoding.  (Multi-Head Attention), and Encoder and Decoder layers. And in the end we combine Encoder and Decoder layers to create the complete Transformer model.\n",
    "\n",
    " \n",
    "Still, if see your <span style=\"color:red\">#### YOUR CODE ####</span>  headline, it is the part you need to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee357c17",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Basic components\n",
    "\n",
    "Create Word Embeddings\n",
    "\n",
    "First of all we need to convert each word in the input sequence to an embedding vector. Embedding vectors will create a more semantic representation of each word.\n",
    "\n",
    "\n",
    "<img src=\"./embedding.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Suppoese each embedding vector is of 512 dimension and suppose our vocab size is 100, then our embedding matrix will be of size 100x512. These marix will be learned on training and during inference each word will be mapped to corresponding 512 d vector. \n",
    "\n",
    "Suppose we have batch size of 2 and sequence length of 12(12 words). The the output will be 2x10x512.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "08b9c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "##hint: you can use nn.Embedding function to create work embedding\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        ###YOUR CODE###\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "         ###YOUR CODE###\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ab1a5352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 512])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a sample with batch size of 2 , sequence length of 12\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "ebed = Embedding(100, 512)\n",
    "embeddings = ebed(src)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bc823",
   "metadata": {},
   "source": [
    "### Position embedding\n",
    "\n",
    "In order for the model to make sense of the sentence,we should not only know \"what does the word mean\" (by above embedding approach), but also need to know \"what is the position of the word in the sentence\"?\n",
    "\n",
    "So, next we need to create position embedding, to include the word position information to the model.\n",
    "\n",
    "\n",
    "In \"attention is all you need paper\" author used the following functions to create positional encoding. On odd time steps a cosine function is used and in even time steps a sine function is used.\n",
    "\n",
    "\n",
    "<img src=\"./position_embedding.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "\n",
    "Positinal embedding will generate a matrix of similar to embedding matrix. It will create a matrix of dimension sequence length x embedding dimension. For each token(word) in sequence, we will find the embedding vector which is of dimension 1 x 512 and it is added with the correspondng positional vector which is of dimension 1 x 512 to get 1 x 512 dim out for each word/token.\n",
    "\n",
    "for eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512. Then we will have embedding vector of dimension 2 x 12 x 512. Similarly we will have positional encoding vector of dimension 2 x 12 x 512. Then we add both.\n",
    "\n",
    "<img src=\"./position_embedding2.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f499b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you have parameters in your model, which should be saved and restored in the state_dict,\n",
    "# but not trained by the optimizer, you should register them as buffers.\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,max_seq_len,embed_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len        : length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_model_dim\n",
    "        pe = torch.zeros(max_seq_len,self.embed_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,self.embed_dim,2):\n",
    "                ###YOUR CODE###\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
    "                ###YOUR CODE###\n",
    "\n",
    "        pe = pe.unsqueeze(0) \n",
    "    \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "      \n",
    "        # make embeddings relatively larger\n",
    "        print(\"x\",x.shape)\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        print(\"x after\",x.shape)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:,:seq_len], requires_grad=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f181a",
   "metadata": {},
   "source": [
    "### Block 1:   (Multi-Head) Attention\n",
    "\n",
    "To build self attention, the first step is to initial metrics WQ,WV,WK. Each of the dim is [embed_dim, head_dim].\n",
    "Here, we’ve initialized three independent linear layers that apply matrix multiplication to the embedding vectors to produce tensors of shape [batch_size, seq_len, head_dim], where head_dim is the number of dimensions we are projecting into. Although head_dim does not have to be smaller than the number of embedding dimensions of the tokens (embed_dim), in practice it is chosen to be a multiple of embed_dim so that the computation across each head is constant.\n",
    "\n",
    "\n",
    "<img src=\"./self_attention1.png\" width=\"400\" height=\"200\">\n",
    "\n",
    "Then Compute attention scores. We determine how much the query and key vectors relate to each other using a similarity function. As the name suggests, the similarity function for scaled dot-product attention is the dot product, computed efficiently using matrix multiplication of the embeddings. Queries and keys that are similar will have a large dot product, while those that don’t share much in common will have little to no overlap. The outputs from this step are called the attention scores, and for a sequence with n input tokens there is a corresponding n x n matrix of attention scores.\n",
    "\n",
    "<img src=\"./self_attention2.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4eff54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    dim_k = query.size(-1)\n",
    "    # torch.bmm is batch matrix - matrix multiplication. \n",
    "    # Basically a dot product.\n",
    "    ###YOUR CODE####\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) \n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    scaled_score = torch.bmm(weights, value)\n",
    "    print(scaled_score.shape)\n",
    "    ###YOUR CODE####\n",
    "    return scaled_score\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee9008c",
   "metadata": {},
   "source": [
    "Above is the single attetion, now we need to impelment multi-head attetion:\n",
    "    \n",
    "<img src=\"./multi-head-attention.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "\n",
    "<span style=\"color:red\">#### Below, you only need to compute the attention scores  ####</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "822aa411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: dimension of embeding vector output\n",
    "            n_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim    #512 dim\n",
    "        self.n_heads = n_heads        #8\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  each key,query, value will be of 64d\n",
    "       \n",
    "        #key,query and value matrixes    #64 x 64   \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512\n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n",
    "\n",
    "    def forward(self,key,query,value,mask=None):    #batch_size x sequence_length x embedding_dim    # 2 x 12 x 512\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key : key vector\n",
    "           query : query vector\n",
    "           value : value vector\n",
    "           mask: mask for decoder\n",
    "        \n",
    "        Returns:\n",
    "           output vector from multihead attention\n",
    "        \"\"\"\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        \n",
    "        # query dimension can change in decoder during inference. \n",
    "        # so we cant take general seq_length\n",
    "        seq_length_query = query.size(1)\n",
    "        \n",
    "        # 32x10x512\n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (2x12x8x64)\n",
    "        query = query.view(batch_size, seq_length_query, self.n_heads, self.single_head_dim) #(2x12x8x64)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(2x12x8x64)\n",
    "       \n",
    "        k = self.key_matrix(key)     # (32x10x8x64)\n",
    "        q = self.query_matrix(query)   \n",
    "        v = self.value_matrix(value)\n",
    "\n",
    "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (2 x 12 x 10 x 64)\n",
    "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "       \n",
    "    \n",
    "        \n",
    "        # computes attention\n",
    "        # adjust key for matrix multiplication\n",
    "        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(2 x 8 x 64 x 12)\n",
    "       \n",
    "        ###YOUR CODE#### \n",
    "        ##Your need to compute the attention score here\n",
    "        product = torch.matmul(q, k_adjusted)  #(2 x 8 x 12 x 64) x (2 x 8 x 64 x 12) = #(2x8x10x12)\n",
    "      \n",
    "        # fill those positions of product matrix as (-1e20) where mask positions are 0\n",
    "        if mask is not None:\n",
    "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "\n",
    "        #divising by square root of key dimension\n",
    "        product = product / math.sqrt(self.single_head_dim) # / sqrt(64)\n",
    "\n",
    "        #applying softmax\n",
    "        scores = F.softmax(product, dim=-1)\n",
    " \n",
    "        #mutiply with value matrix\n",
    "        scores = torch.matmul(scores, v)  ##(32x8x 10x 10) x (2 x 8 x 12 x 64) = (2 x 8 x 12 x 64) \n",
    "\n",
    "        ###YOUR CODE####\n",
    "            \n",
    "            \n",
    "        #concatenated output\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length_query, self.single_head_dim*self.n_heads)  # (2x8x12x64) -> (2x12x8x64)  -> (2,8,512)\n",
    "        \n",
    "        output = self.out(concat) #(2,12,512) -> (2,12,512)\n",
    "       \n",
    "        return output\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2aba4",
   "metadata": {},
   "source": [
    "### Encoder:\n",
    "\n",
    "Now we need to establish the encoder part. There are four steps:\n",
    "\n",
    "Step 1: First input(padded tokens corresponding to the sentence) get passes through embedding layer and positional encoding layer.\n",
    "\n",
    "Step 2: As discussed above it will passed through the multihead attention layer and creates useful representational matrix as output.\n",
    "\n",
    "Step 3: Next we have a normalization and residual connection. The output from multihead attention is added with its input and then normalized.\n",
    "\n",
    "Step 4: Next we have a feed forward layer and a then normalization layer with residual connection from input(input of feed forward layer) where we passes the output after normalization though it and finally gets the output of encoder.\n",
    "\n",
    "\n",
    "<img src=\"./encoder.png\" width=\"300\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf69349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim) \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,key,query,value):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           norm2_out: output of transformer block\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        attention_out = self.attention(key,query,value)  #2x12x512\n",
    "        attention_residual_out = attention_out + value  #2x12x512\n",
    "        norm1_out = self.dropout1(self.norm1(attention_residual_out)) #2x12x512\n",
    "\n",
    "        feed_fwd_out = self.feed_forward(norm1_out) #2x12x512 -> #2x12x2048 ->2x12x512\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out #2x12x512\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out)) #2x12x512\n",
    "\n",
    "        return norm2_out\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "        n_heads: number of heads in multihead attention\n",
    "        \n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        ###YOUR CODE####\n",
    "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
    "        ###YOUR CODE####\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out,out,out)\n",
    "        return out  #32x10x512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78c096",
   "metadata": {},
   "source": [
    "###   Decoder \n",
    "\n",
    "\n",
    "Now, let's build the decoder of transformer. We will use the output of encoder to generate key and value vectors for the decoder.There are two kinds of multi head attention in the decoder.One is the decoder attention and other is the encoder decoder attention. \n",
    "\n",
    "\n",
    "<img src=\"./decoder.png\" width=\"200\" height=\"80\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fbfcb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
    "        \n",
    "    \n",
    "    def forward(self, key, query, x,mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attention \n",
    "        Returns:\n",
    "           out: output of transformer block\n",
    "    \n",
    "        \"\"\"\n",
    "        \n",
    "        #we need to pass mask mask only to fst attention\n",
    "        ##YOUR CODE###\n",
    "        attention = self.attention(x,x,x,mask=mask) #2x12x512\n",
    "        value = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(key, query, value)\n",
    "        ##YOUR CODE###\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7914d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           target_vocab_size: vocabulary size of taget\n",
    "           embed_dim: dimension of embedding\n",
    "           seq_len : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, enc_out, mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector from target\n",
    "            enc_out : output from encoder layer\n",
    "            trg_mask: mask for decoder self attention\n",
    "        Returns:\n",
    "            out: output vector\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        x = self.word_embedding(x)  #2x12x512\n",
    "        x = self.position_embedding(x) #2x12x512\n",
    "        x = self.dropout(x)\n",
    "     \n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, x, enc_out, mask) \n",
    "\n",
    "        out = F.softmax(self.fc_out(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055f732",
   "metadata": {},
   "source": [
    "### Wrap it up, to buid the entire Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e7e82d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           embed_dim:  dimension of embedding \n",
    "           src_vocab_size: vocabulary size of source\n",
    "           target_vocab_size: vocabulary size of target\n",
    "           seq_length : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.target_vocab_size = target_vocab_size\n",
    "\n",
    "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        \n",
    "        \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask    \n",
    "\n",
    "    def decode(self,src,trg):\n",
    "        \"\"\"\n",
    "        for inference\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out_labels : returns final prediction of sequence\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "        out_labels = []\n",
    "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
    "        #outputs = torch.zeros(seq_len, batch_size, self.target_vocab_size)\n",
    "        out = trg\n",
    "        for i in range(seq_len): #12\n",
    "            out = self.decoder(out,enc_out,trg_mask) #bs x seq_len x vocab_dim\n",
    "            # taking the last token\n",
    "            out = out[:,-1,:]\n",
    "     \n",
    "            out = out.argmax(-1)\n",
    "            out_labels.append(out.item())\n",
    "            out = torch.unsqueeze(out,axis=0)\n",
    "          \n",
    "        \n",
    "        return out_labels\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out: final vector which returns probabilities of each target word\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src)\n",
    "   \n",
    "        outputs = self.decoder(trg, enc_out, trg_mask)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c332b2f",
   "metadata": {},
   "source": [
    "### Create some samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ae272f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12]) torch.Size([2, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding_layer): Embedding(\n",
       "      (embed): Embedding(11, 512)\n",
       "    )\n",
       "    (positional_encoder): PositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (word_embedding): Embedding(11, 512)\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderBlock(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (transformer_block): TransformerBlock(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (query_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (key_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (value_matrix): Linear(in_features=64, out_features=64, bias=False)\n",
       "            (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): ReLU()\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.2, inplace=False)\n",
       "          (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=512, out_features=11, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_layers = 6\n",
    "seq_length= 12\n",
    "\n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1], \n",
    "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
    "\n",
    "print(src.shape,target.shape)\n",
    "model = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, \n",
    "                    target_vocab_size=target_vocab_size, seq_length=seq_length,\n",
    "                    num_layers=num_layers, expansion_factor=4, n_heads=8)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1f6292d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_r/h4p_j1bj32x5n57v17k948w80000gn/T/ipykernel_28120/294460809.py:47: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(self.fc_out(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.4025018215179443\n",
      "Epoch: 2, Loss: 2.388179063796997\n",
      "Epoch: 3, Loss: 2.415064811706543\n",
      "Epoch: 4, Loss: 2.4056813716888428\n",
      "Epoch: 5, Loss: 2.400160789489746\n",
      "Epoch: 6, Loss: 2.3532369136810303\n",
      "Epoch: 7, Loss: 2.3556859493255615\n",
      "Epoch: 8, Loss: 2.379387855529785\n",
      "Epoch: 9, Loss: 2.350984573364258\n",
      "Epoch: 10, Loss: 2.338085174560547\n",
      "Epoch: 11, Loss: 2.378542423248291\n",
      "Epoch: 12, Loss: 2.3276898860931396\n",
      "Epoch: 13, Loss: 2.337876081466675\n",
      "Epoch: 14, Loss: 2.3059282302856445\n",
      "Epoch: 15, Loss: 2.288414239883423\n",
      "Epoch: 16, Loss: 2.2943315505981445\n",
      "Epoch: 17, Loss: 2.3021605014801025\n",
      "Epoch: 18, Loss: 2.352037191390991\n",
      "Epoch: 19, Loss: 2.2697043418884277\n",
      "Epoch: 20, Loss: 2.2466962337493896\n",
      "Epoch: 21, Loss: 2.1889684200286865\n",
      "Epoch: 22, Loss: 2.25447678565979\n",
      "Epoch: 23, Loss: 2.218231439590454\n",
      "Epoch: 24, Loss: 2.278759241104126\n",
      "Epoch: 25, Loss: 2.2357537746429443\n",
      "Epoch: 26, Loss: 2.241239309310913\n",
      "Epoch: 27, Loss: 2.220705509185791\n",
      "Epoch: 28, Loss: 2.206367254257202\n",
      "Epoch: 29, Loss: 2.2334845066070557\n",
      "Epoch: 30, Loss: 2.2693369388580322\n",
      "Epoch: 31, Loss: 2.2307698726654053\n",
      "Epoch: 32, Loss: 2.1834893226623535\n",
      "Epoch: 33, Loss: 2.1702358722686768\n",
      "Epoch: 34, Loss: 2.174652576446533\n",
      "Epoch: 35, Loss: 2.180206537246704\n",
      "Epoch: 36, Loss: 2.1355538368225098\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src, target)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, target_vocab_size), target[:, :]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, target)\n",
    "    loss = criterion(output.contiguous().view(-1, target_vocab_size), target.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c456ac1",
   "metadata": {},
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dacde202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
