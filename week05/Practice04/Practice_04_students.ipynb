{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815ab61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/binggong/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/binggong/anaconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/binggong/anaconda3/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <5AA8DD3D-A2CC-31CA-8060-88B4E9C18B09> /Users/binggong/anaconda3/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <42C20470-F879-3070-BF42-74C7B230A1A1> /Users/binggong/anaconda3/lib/python3.10/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports the print function from newer versions of python\n",
    "from __future__ import print_function\n",
    "#PyTorch Libarary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# The Random module implements pseudo-random number generators\n",
    "import random \n",
    "\n",
    "# Numpy is the main package for scientific computing with Python. \n",
    "# This will be one of our most used libraries in this class\n",
    "import numpy as np\n",
    "\n",
    "# The Time library helps us time code runtimes\n",
    "import time\n",
    "\n",
    "# The skimage is used for the image processing tasks\n",
    "from skimage import data\n",
    "from skimage.color import rgb2hsv, hsv2rgb\n",
    "\n",
    "# Matplotlib is a useful plotting library for python \n",
    "import matplotlib.pyplot as plt\n",
    "# This code is to make matplotlib figures appear inline in the\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a335724",
   "metadata": {},
   "source": [
    "## PyTorch Basic \n",
    "\n",
    "In this excise, you will start to get familiar with PyTorch.  PyTorch is a machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. \n",
    "\n",
    "\n",
    "The target of the this tutorial is to :\n",
    "\n",
    "   * Understand the concept of   <span style=\"color:green\">Tensor</span> \n",
    "   * Know how to create a tensor by PyTorch\n",
    "   * Know how to do gradents calcuation by PyTorch \n",
    "   * Know how to do backprogatation by PyTorch\n",
    "   * Be able to build data pipline\n",
    "   * Familiar with training a simple neural network by PyTorch\n",
    "\n",
    "\n",
    "Still, if see your <span style=\"color:red\">#### YOUR CODE ####</span>  headline, it is the part you need to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da58f9",
   "metadata": {},
   "source": [
    "### Tensor \n",
    "在开始之前，我们需要了解什么是Tensor,它的基本概念。\n",
    "\n",
    "Tensor是PyTorch最基本的操作对象，表示的是一个多维矩阵，核心是数据容器。\n",
    "\n",
    "tensor 与 numpy 相对应，可与numpy 相互转换。 tensor 可以轻易地进行卷积、激活、上次样和下采样、求导等操作。但是numpy不具备此特征。numpy数组需要转化成tensor格式才可以进行以上操作。\n",
    "\n",
    "另外，现代许多技术中需要使用GPU大量并行计算，提高速度，很遗憾，numpy不能在支持在一些GPU运算平台如CUDA上并行计算。Pytorch中最为重要的一个技术，Tensor，便解决了该问题。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6437bb",
   "metadata": {},
   "source": [
    "### Create a tensor\n",
    "\n",
    "那么如何建立tensor？ 可以通过以下方法：\n",
    "* 可以通过tensor.tensor 命令直接建立tensor（https://pytorch.org/docs/stable/tensors.html）\n",
    "* 可以读入numpy数据，转化成tensor （https://pytorch.org/docs/stable/generated/torch.from_numpy.html）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c925b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#例如：通过tensor.tensor建立\n",
    "x = torch.tensor(1.)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "### YOUR CODE###\n",
    "y = \n",
    "### YOUR CODE###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569b8f7",
   "metadata": {},
   "source": [
    "另外， tensor也可以同时转换成numpy.\n",
    "请尝试将上面tensor y 转换成 numpy 格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5671667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "####YOUR CODE ####\n",
    "z = \n",
    "####YOUR CODE ####\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f536718",
   "metadata": {},
   "source": [
    "### 求梯度\n",
    "\n",
    "下面的例子，我们将通过PyTorch计算梯度：\n",
    "\n",
    "首先，我们别建立tensor x等于1， w等于2，以及b等于3的三个tensor，然后，一个简单的回归模型 y = wx + b，并对tensor x 进行求导。\n",
    "\n",
    "hint: 这里我们需要用requires_grad，将其设置为True，则表示该Tensor需要求导。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87522c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create tensors.\n",
    "####YOUR CODE####\n",
    "x = \n",
    "w = \n",
    "b = \n",
    "####YOUR CODE####\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "####YOUR CODE####\n",
    "\n",
    "####YOUR CODE####\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5fa9f",
   "metadata": {},
   "source": [
    "### Backpropagation(反向传播算法)\n",
    "接下来，我们将通过另一个例子，来完成完成反向传播算法，更新模型参数。\n",
    "下面x为数据的输入，y为输出。x为10x3维数据， 10维sample的个数，3为维数。同理，y是一个拥有2个变量的输出数据。\n",
    "建立一个简单的线性全连接网络。\n",
    "\n",
    "接下来，你需要完成：\n",
    "    \n",
    "    * 定义loss funciton (MSE)\n",
    "    * 选择优化策略（SGD）, 并将learning rate 设置为0.01\n",
    "    * 向前计算，计算模型的输出。\n",
    "    * 计算loss\n",
    "    * 求参数的梯度。\n",
    "    * 对参数进行一步的优化、更新\n",
    "    * 用更新后参数，进行前向计算，得到模型输出，并计算新的loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5f5705e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.5222,  0.1630,  0.2347],\n",
      "        [-0.0144, -0.2735, -0.5373]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.0913, -0.2498], requires_grad=True)\n",
      "loss:  1.0719209909439087\n",
      "dL/dw:  tensor([[-0.3707, -0.0644,  0.0947],\n",
      "        [ 0.6459, -0.3069, -0.3861]])\n",
      "dL/db:  tensor([-0.6018,  0.0332])\n",
      "loss after 1 step optimization:  1.0602302551269531\n"
     ]
    }
   ],
   "source": [
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer neural network \n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "####YOUR CODE####\n",
    "criterion = \n",
    "optimizer = \n",
    "####YOUR CODE####\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "####YOUR CODE####\n",
    "loss = \n",
    "####YOUR CODE####\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "####YOUR CODE####\n",
    "\n",
    "####YOUR CODE####\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "####YOUR CODE####\n",
    "optimizer.step()\n",
    "####YOUR CODE####\n",
    "\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6eb78a",
   "metadata": {},
   "source": [
    "如果以上顺利完成，你应该得到以下结果：\n",
    "\n",
    "* 参数更新前的loss应该为：loss:  1.0719209909439087\n",
    "* 参数更新后的loss应该为：1.0602302551269531"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a8d5d",
   "metadata": {},
   "source": [
    "### Inputs pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968285f",
   "metadata": {},
   "source": [
    "In cases where the training dataset is rather small and can be loaded as a tensor into the memory as above, we can directly use this tensor for training. In typical use cases, however, when the dataset is too large to fit into the computer memory, we will need to load the data from the main storage device (for example, the hard drive or solid-state drive) in chunks, that is, batch by batch. In addition, we may need to construct a data-processing pipeline to apply certain transformations and preprocessing steps to our data. So it is necessary to build an an input pipline to read the data batch by batch to feed into the deep neural network for training.\n",
    "\n",
    "\n",
    "In this case, we will use real world application, and get the MINST benchmark dataset from `torchvision` pacakge. \n",
    "MINST benchmark dataset is a large database of handwritten digits that is commonly used for training various image processing systems. (for more information about the dataset https://git-disl.github.io/GTDLBench/datasets/mnist_datasets/). \n",
    "\n",
    "The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision. You can use use ` torchvision.datasets` to download various datasets (https://pytorch.org/vision/stable/datasets.html#built-in-datasets)\n",
    "\n",
    "For instance, in the following, we download the MINST test dataset into the folder `./data` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14ddc381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3967148.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 24596079.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 1876873.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 3810105.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download and construct MINST dataset.\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                             train=False, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "df1502d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Fetch one data pair (read data from disk).\n",
    "image, label = test_dataset[1]\n",
    "print (image.size())\n",
    "print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c7b1bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14a099240>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e220fe",
   "metadata": {},
   "source": [
    "Next you need to load a `batch` of data for each iteration, in this case,  you should use the class`'torch.utils.data.DataLoader` https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "Read the above document carefully, and try to use the dataload to read 64 samples per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ca57a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "\n",
    "#When iteration starts, queue and thread start to load data from files.\n",
    "# Mini-batch images and labels.\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in test_loader:\n",
    "    # Training code should be written here.\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af530711",
   "metadata": {},
   "source": [
    "###  Logistic regression \n",
    "\n",
    "Next, we will build a real application for image classification by using the MINST dataset. Before, we used test dataset for MINST, now we will load the both training and test dataset.\n",
    "\n",
    "Training dataset is mainly for train the neural network model, while the test dataset is used for evaluate the performance of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d9a298",
   "metadata": {},
   "source": [
    "First we need to define some hyper-parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "279bc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 28 * 28    # 784\n",
    "num_classes = 10 #MNIST contains 10 classes \n",
    "#An epoch is when all the training data is used at once and is defined as the total number of iterations of\n",
    "# all the training data in one cycle for training the machine learning model\n",
    "num_epochs = 5 \n",
    "batch_size = 64 # how many samples for iteration\n",
    "learning_rate = 0.001 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e84b3c",
   "metadata": {},
   "source": [
    "You need to build an training and testing dataset pipline seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1ee7c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 4733165.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 69304.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 1685245.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2911589.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MNIST dataset (images and labels)\n",
    "####YOUR CODE #### \n",
    "train_dataset = \n",
    "\n",
    "####YOUR CODE #### \n",
    "test_dataset = \n",
    "\n",
    "# Data loader (input pipeline)\n",
    "\n",
    "####YOUR CODE #### \n",
    "train_loader = \n",
    "####YOUR CODE #### \n",
    "\n",
    "####YOUR CODE #### \n",
    "test_loader = \n",
    "####YOUR CODE #### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72f546",
   "metadata": {},
   "source": [
    "Build an linear regression model for classifikcaiton, you need to \n",
    "* build a model  (use, nn.Linear function)\n",
    "* define the loss fuciotn \n",
    "* define the optiser ( use SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e7e7de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "####YOUR CODE #### \n",
    "model = \n",
    "####YOUR CODE #### \n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "####YOUR CODE #### \n",
    "criterion = \n",
    "####YOUR CODE #### \n",
    "\n",
    "####YOUR CODE #### \n",
    "optimizer = \n",
    "####YOUR CODE #### \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2f8c2",
   "metadata": {},
   "source": [
    "Run the following code to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30618d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 2.2343\n",
      "Epoch [1/5], Step [200/938], Loss: 2.1272\n",
      "Epoch [1/5], Step [300/938], Loss: 1.9988\n",
      "Epoch [1/5], Step [400/938], Loss: 1.9546\n",
      "Epoch [1/5], Step [500/938], Loss: 1.8779\n",
      "Epoch [1/5], Step [600/938], Loss: 1.8418\n",
      "Epoch [1/5], Step [700/938], Loss: 1.7497\n",
      "Epoch [1/5], Step [800/938], Loss: 1.7112\n",
      "Epoch [1/5], Step [900/938], Loss: 1.6521\n",
      "Epoch [2/5], Step [100/938], Loss: 1.5924\n",
      "Epoch [2/5], Step [200/938], Loss: 1.4643\n",
      "Epoch [2/5], Step [300/938], Loss: 1.5164\n",
      "Epoch [2/5], Step [400/938], Loss: 1.3892\n",
      "Epoch [2/5], Step [500/938], Loss: 1.4089\n",
      "Epoch [2/5], Step [600/938], Loss: 1.3408\n",
      "Epoch [2/5], Step [700/938], Loss: 1.3758\n",
      "Epoch [2/5], Step [800/938], Loss: 1.3606\n",
      "Epoch [2/5], Step [900/938], Loss: 1.2332\n",
      "Epoch [3/5], Step [100/938], Loss: 1.2710\n",
      "Epoch [3/5], Step [200/938], Loss: 1.1405\n",
      "Epoch [3/5], Step [300/938], Loss: 1.2316\n",
      "Epoch [3/5], Step [400/938], Loss: 1.1873\n",
      "Epoch [3/5], Step [500/938], Loss: 1.1318\n",
      "Epoch [3/5], Step [600/938], Loss: 1.0709\n",
      "Epoch [3/5], Step [700/938], Loss: 1.1940\n",
      "Epoch [3/5], Step [800/938], Loss: 1.0147\n",
      "Epoch [3/5], Step [900/938], Loss: 1.0272\n",
      "Epoch [4/5], Step [100/938], Loss: 1.0854\n",
      "Epoch [4/5], Step [200/938], Loss: 1.0319\n",
      "Epoch [4/5], Step [300/938], Loss: 1.0134\n",
      "Epoch [4/5], Step [400/938], Loss: 0.8777\n",
      "Epoch [4/5], Step [500/938], Loss: 1.0289\n",
      "Epoch [4/5], Step [600/938], Loss: 1.0552\n",
      "Epoch [4/5], Step [700/938], Loss: 1.0211\n",
      "Epoch [4/5], Step [800/938], Loss: 0.9923\n",
      "Epoch [4/5], Step [900/938], Loss: 1.0382\n",
      "Epoch [5/5], Step [100/938], Loss: 0.9044\n",
      "Epoch [5/5], Step [200/938], Loss: 0.8142\n",
      "Epoch [5/5], Step [300/938], Loss: 0.9125\n",
      "Epoch [5/5], Step [400/938], Loss: 0.9328\n",
      "Epoch [5/5], Step [500/938], Loss: 0.9365\n",
      "Epoch [5/5], Step [600/938], Loss: 0.9087\n",
      "Epoch [5/5], Step [700/938], Loss: 0.7804\n",
      "Epoch [5/5], Step [800/938], Loss: 0.8120\n",
      "Epoch [5/5], Step [900/938], Loss: 1.0284\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, input_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3973bbd",
   "metadata": {},
   "source": [
    "use the trained model, and using the test dataset to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33a3dbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 84.7300033569336 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency), so you have to use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e61e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5388e",
   "metadata": {},
   "source": [
    "### Read more from the official website : https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "If you want to explore more and get assistant of using PyTorch, it is highly recommend to read the official document for more information"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
